#summary UpRight TBD

2009.6.2.txt -- UpRight releast TBD list

Allen 
  Get proposal done
  Idetnify critical subset for core; do that ASAP

Yang 
   Understand current state of HDFS and make TBD list
   Update web pages and examples

Allen + Yang 
      -- put time estimate on each item on list
      -- prioritize items on list

Code -- UpRight
  - View change works
     -- Reimplement view change ack (similar to filtered messages)      
     -- Re-Implement define new view code
     -- code to kill order node; load from stable
     -- view change timers
     -- server shim -- write checkpoint to disk before sending to order
     -- server shim -- load checkpoints from disk
  - 1 day allen clean up comments
  - optimize l=0 case to use single mac
  - DoS protection on quorum gathering
  - Performance tuning (Mirco on C code -- too many threads)
  - Refactor  -- filter, order, server, client should not extend base node  --> can put on same node in same process easily
  - Configuration
  - extend glue to seed the random for each batch; defensive programming against concurrency
  - Feature: change b or c on the fly

Code -- C UpRight
   - add filter to replace signatures w/ hashes
   - security, crypto library
   - view change timing
   - view change message (hashes + bookeeping messages)
   - stable
   - dos protection
   - peformance tuning

Code -- HDFS-UpRight
  - Figure it out
  - Security for data node adds -- get data node ID from key not IP address
      restrict who can claim to be data node
  - feature: add/remove clients (both clients and data nodes)

Web page -- UpRight
  - License
      -- Today GPL
      -- Before we send it out to users, consider (a) moving shim and HDFS mods
           to Apache but leaving core to GPL or (b) moving all to Apache
      -- What are licenses of libraries on which we depend? (What are issues wrt linking
           in unmodified libraries)
  - Refine text per mike's comments
  - download/install instructions -- external libraries issues
     -- crypto (flexicore)
     -- two others jss4.jar (?) codec.jar (?)
     -- keep local version; have chron job that tracks these sites
  - Example
      -- Update app -- random and real time
      -- Working unreplicated ("Working" = instructions + sample code + sample config)
      -- Working c=1, b=1 (combine RQx4, Ox4, Sx3 w/o fairness)
      -- *Working c=1, b=0
      -- *Working c=2, b=1
      -- *Working c=1, b=1 (signatures)
      -- *Working c=1, b=1 (split RQx4, Ox4, Sx3 on different machines; w/o fairness)
      -- *Working c=1, b=1 (RQ w/ fairness)
      -- *Working c=1, b=1 custom app + shim
   - Add performance microbenchmarks
       -- c=1 b=1 0-byte, 1K x  r/w request, read only request x latency, throughput
       -- c=1 b=0
       -- c=3, b=1

Web pages -- HDFS-UpRight
   - Overview
   - Internal changes to HDFS
   - Example
       -- Working c=1, b=0
       -- Working c=1, b=1
       -- Working c=1, b=1, b_datanode=2
   - define benchmarks
       -- throughput of reads/writes for the 3 main configs
       -- recovery demonstration -- single name node crash, no recover
       -- recovery time -- single name node crash/recover
                                  -- power outage (replicated name node crash/recover)
   - implement and run benchmarks  

Web page
   - svn access (move our repository to google?)
   - set up "notify on checkin" for some of us
   - is there any way to track who/how often downloads happen?



Demo
   - Internal use
      -- switch to replicated order nodes
   - 24x7 web demo
      -- start on machine reboot
      -- watchdog restart
      -- need more machines


Communication
   - Facebook -- Dhruba, Harry
   - Amazon -- Werner, Theimer, Hamilton, Zheng
   - Google -- Bershad
   - Navy -- Dahlin
